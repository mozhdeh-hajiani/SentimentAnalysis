{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNUcgG660uzWIrnYYz5Jiu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["bow balanced LR"],"metadata":{"id":"hlsdiClNqT7d"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0gVNjKDqJr8"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from sklearn.utils import resample"]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","import pandas as pd\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    filtered_text = [normalize_word(w) for w in text_tokens if w not in stop_words]\n","    return \" \".join(filtered_text)\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","def stemming(data):\n","    return \" \".join([stemmer.stem(word) for word in data.split()])\n","\n","# Train data\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","# Visualizations\n","plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","majority_class = text_df[text_df['sentiment'] == 1]\n","minority_class = text_df[text_df['sentiment'] == 0]\n","\n","# Upsample minority class\n","minority_upsampled = resample(minority_class, replace=True,n_samples=len(majority_class),random_state=35)\n","\n","text_df = pd.concat([majority_class, minority_upsampled])\n","\n","#  TF-IDF\n","vectorizer = CountVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train/test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","logreg_model = LogisticRegression(class_weight='balanced', solver='liblinear')  # Set class_weight to balanced\n","logreg_model.fit(x_train, y_train)\n","logreg_pred = logreg_model.predict(x_test)\n","logreg_acc = accuracy_score(y_test, logreg_pred)\n","f1_macro = f1_score(y_test, logreg_pred, average='macro')\n","\n","print(\"Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","print(confusion_matrix(y_test, logreg_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, logreg_pred))\n","\n","# Hyperparameter tuning\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","}\n","\n","grid = GridSearchCV(LogisticRegression(class_weight='balanced', solver='liblinear'), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","print(\"Best parameter:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","logreg_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Logistic Regression - Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","best_logreg_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Logistic Regression - F1 Macro score: {:.2f}\".format(best_logreg_f1_macro))\n","\n","# data processing on evaluation set\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(lambda x: stemming(x))\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/bow-balance-logistic-regression.csv', index=False)"],"metadata":{"id":"tq4Nd2GDqPg-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["bow balanced naive bayse"],"metadata":{"id":"cFCKfyauqZz3"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from sklearn.utils import resample\n","from sklearn.naive_bayes import MultinomialNB"],"metadata":{"id":"CvM__CXHqc3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","import pandas as pd\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    filtered_text = [word for word in text_tokens if word not in stop_words]\n","    return \" \".join(filtered_text)\n","\n","# Train data\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","\n","# Visualizations\n","plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","majority_class = text_df[text_df['sentiment'] == 1]\n","minority_class = text_df[text_df['sentiment'] == 0]\n","\n","# Upsample minority class\n","minority_upsampled = resample(minority_class,replace=True,n_samples=len(majority_class),random_state=35)\n","\n","text_df = pd.concat([majority_class, minority_upsampled])\n","\n","# TF-IDF\n","vectorizer = CountVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train/test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","nb_model = MultinomialNB()\n","nb_model.fit(x_train, y_train)\n","nb_pred = nb_model.predict(x_test)\n","nb_acc = accuracy_score(y_test, nb_pred)\n","f1_macro = f1_score(y_test, nb_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(nb_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","print(confusion_matrix(y_test, nb_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, nb_pred))\n","\n","# Hyperparameter tuning\n","param_grid = {\n","    'alpha': [0.1, 0.5, 1.0, 1.5]\n","}\n","\n","grid = GridSearchCV(MultinomialNB(), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","print(\"Best parameter:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","nb_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Naive Bayes - Test accuracy: {:.2f}%\".format(nb_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","best_nb_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Naive Bayes - F1 Macro score: {:.2f}\".format(best_nb_f1_macro))\n","\n","# data processing on evaluation set\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/bow-balance-naive-bayes.csv', index=False)"],"metadata":{"id":"qtgw9OgZqdR4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["bow balanced SGD"],"metadata":{"id":"CrBmds8ZqmQY"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from sklearn.utils import resample\n","from sklearn.linear_model import SGDClassifier"],"metadata":{"id":"-Ge8iPQIqpcB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","import pandas as pd\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    filtered_text = [word for word in text_tokens if word not in stop_words]\n","    return \" \".join(filtered_text)\n","\n","# Train data\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","\n","# Visualizations\n","plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","\n","majority_class = text_df[text_df['sentiment'] == 1]\n","minority_class = text_df[text_df['sentiment'] == 0]\n","\n","# Upsample minority class\n","minority_upsampled = resample(minority_class,replace=True,n_samples=len(majority_class),random_state=35)\n","\n","\n","text_df = pd.concat([majority_class, minority_upsampled])\n","\n","# Vectorize\n","vectorizer = CountVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train/test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","sgd_model = SGDClassifier(loss='log_loss', random_state=35)\n","sgd_model.fit(x_train, y_train)\n","sgd_pred = sgd_model.predict(x_test)\n","sgd_acc = accuracy_score(y_test, sgd_pred)\n","f1_macro = f1_score(y_test, sgd_pred, average='macro')\n","\n","print(\"Test accuracy: {:.2f}%\".format(sgd_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","print(confusion_matrix(y_test, sgd_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, sgd_pred))\n","\n","# Hyperparameter tuning\n","param_grid = {\n","    'alpha': [1e-4, 1e-3, 1e-2, 1e-1],\n","    'max_iter': [1000, 2000, 3000],\n","    'tol': [1e-3, 1e-4]\n","}\n","\n","grid = GridSearchCV(SGDClassifier(loss='log_loss'), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","print(\"Best parameters:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","sgd_acc_best = accuracy_score(y_test, y_pred)\n","print(\"Best SGD - Test accuracy: {:.2f}%\".format(sgd_acc_best * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","best_sgd_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best SGD - F1 Macro score: {:.2f}\".format(best_sgd_f1_macro))\n","\n","# data processing on evaluation set\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/bow-balance-sgd.csv', index=False)"],"metadata":{"id":"7ojoJ8AVqqh5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["bow balanced svm"],"metadata":{"id":"DOD4mOqgqyhq"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from sklearn.utils import resample"],"metadata":{"id":"QmjMEoROq2oc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Mount Google Drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    filtered_text = [normalize_word(w) for w in text_tokens if w not in stop_words]\n","    return \" \".join(filtered_text)\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","def stemming(data):\n","    return \" \".join([stemmer.stem(word) for word in data.split()])\n","\n","# Train data\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","# Visualizations\n","plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","majority_class = text_df[text_df['sentiment'] == 1]\n","minority_class = text_df[text_df['sentiment'] == 0]\n","\n","# Upsample minority class\n","minority_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class),random_state=35)\n","text_df = pd.concat([majority_class, minority_upsampled])\n","\n","# Vectorize\n","vectorizer = CountVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train/test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","SVCmodel = LinearSVC(class_weight='balanced')\n","SVCmodel.fit(x_train, y_train)\n","svc_pred = SVCmodel.predict(x_test)\n","svc_acc = accuracy_score(y_test, svc_pred)\n","f1_macro = f1_score(y_test, svc_pred, average='macro')\n","\n","print(\"Test accuracy: {:.2f}%\".format(svc_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","print(confusion_matrix(y_test, svc_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, svc_pred))\n","\n","# Hyperparameter tuning\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","}\n","\n","grid = GridSearchCV(LinearSVC(class_weight='balanced'), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","print(\"Best parameter:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","logreg_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Voting Classifier - Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","best_voting_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Voting Classifier - F1 Macro score: {:.2f}\".format(best_voting_f1_macro))\n","\n","# data processing on evaluation set\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(lambda x: stemming(x))\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/bow-balance-svm.csv', index=False)"],"metadata":{"id":"3W84QbU6q3KW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf balanced LR"],"metadata":{"id":"vjihVsBsrCN9"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from sklearn.utils import resample\n"],"metadata":{"id":"SIiA9LB9rC23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","import pandas as pd\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    filtered_text = [normalize_word(w) for w in text_tokens if w not in stop_words]\n","    return \" \".join(filtered_text)\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","def stemming(data):\n","    return \" \".join([stemmer.stem(word) for word in data.split()])\n","\n","# Train data\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","# Visualizations\n","plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","majority_class = text_df[text_df['sentiment'] == 1]\n","minority_class = text_df[text_df['sentiment'] == 0]\n","\n","# Upsample minority class\n","minority_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class),random_state=35)\n","text_df = pd.concat([majority_class, minority_upsampled])\n","\n","# TF-IDF\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train/test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","logreg_model = LogisticRegression(class_weight='balanced', solver='liblinear')  # Set class_weight to balanced\n","logreg_model.fit(x_train, y_train)\n","logreg_pred = logreg_model.predict(x_test)\n","logreg_acc = accuracy_score(y_test, logreg_pred)\n","f1_macro = f1_score(y_test, logreg_pred, average='macro')\n","\n","print(\"Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","print(confusion_matrix(y_test, logreg_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, logreg_pred))\n","\n","# Hyperparameter tuning\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","}\n","\n","grid = GridSearchCV(LogisticRegression(class_weight='balanced', solver='liblinear'), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","print(\"Best parameter:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","logreg_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Logistic Regression - Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","best_logreg_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Logistic Regression - F1 Macro score: {:.2f}\".format(best_logreg_f1_macro))\n","\n","# data processing on evaluation set\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(lambda x: stemming(x))\n","\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/tfidf-balance-logistic-regression.csv', index=False)"],"metadata":{"id":"Gx7HcspWrDfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf balanced naive bayse"],"metadata":{"id":"2Nc2nAWOrMDq"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from sklearn.utils import resample\n","from sklearn.naive_bayes import MultinomialNB"],"metadata":{"id":"TEDJUaTsrPqP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","import pandas as pd\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    filtered_text = [word for word in text_tokens if word not in stop_words]\n","    return \" \".join(filtered_text)\n","\n","# Train data\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","\n","# Visualizations\n","plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","# upsampling\n","majority_class = text_df[text_df['sentiment'] == 1]\n","minority_class = text_df[text_df['sentiment'] == 0]\n","\n","minority_upsampled = resample(minority_class,replace=True, n_samples=len(majority_class), random_state=35)\n","\n","text_df = pd.concat([majority_class, minority_upsampled])\n","\n","# TF-IDF\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train/test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","nb_model = MultinomialNB()\n","nb_model.fit(x_train, y_train)\n","\n","nb_pred = nb_model.predict(x_test)\n","nb_acc = accuracy_score(y_test, nb_pred)\n","f1_macro = f1_score(y_test, nb_pred, average='macro')\n","\n","print(\"Test accuracy: {:.2f}%\".format(nb_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","print(confusion_matrix(y_test, nb_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, nb_pred))\n","\n","# Hyperparameter tuning\n","param_grid = {\n","    'alpha': [0.1, 0.5, 1.0, 1.5]\n","}\n","\n","grid = GridSearchCV(MultinomialNB(), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","print(\"Best parameter:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","nb_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Naive Bayes - Test accuracy: {:.2f}%\".format(nb_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","best_nb_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Naive Bayes - F1 Macro score: {:.2f}\".format(best_nb_f1_macro))\n","\n","# data processing on evaluation set\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/tfidf-balance-naive-bayes.csv', index=False)"],"metadata":{"id":"LAtcQMXarQcy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf balanced SGD"],"metadata":{"id":"CQQ2M_7rrcBR"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from sklearn.utils import resample\n","from sklearn.linear_model import SGDClassifier\n"],"metadata":{"id":"HCsmjpnGrc0f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","import pandas as pd\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    filtered_text = [word for word in text_tokens if word not in stop_words]\n","    return \" \".join(filtered_text)\n","\n","# Train data\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","\n","# Visualizations\n","plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","# upsampling\n","majority_class = text_df[text_df['sentiment'] == 1]\n","minority_class = text_df[text_df['sentiment'] == 0]\n","minority_upsampled = resample(minority_class,replace=True,n_samples=len(majority_class),random_state=35)\n","text_df = pd.concat([majority_class, minority_upsampled])\n","\n","# tfidf\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train/test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","\n","sgd_model = SGDClassifier(loss='log_loss', random_state=35)\n","sgd_model.fit(x_train, y_train)\n","\n","# Predictions and evaluation\n","sgd_pred = sgd_model.predict(x_test)\n","sgd_acc = accuracy_score(y_test, sgd_pred)\n","f1_macro = f1_score(y_test, sgd_pred, average='macro')\n","\n","print(\"Test accuracy: {:.2f}%\".format(sgd_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","print(confusion_matrix(y_test, sgd_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, sgd_pred))\n","\n","# Hyperparameter tuning\n","param_grid = {\n","    'alpha': [1e-4, 1e-3, 1e-2, 1e-1],\n","    'max_iter': [1000, 2000, 3000],\n","    'tol': [1e-3, 1e-4]\n","}\n","\n","grid = GridSearchCV(SGDClassifier(loss='log_loss'), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameters:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","sgd_acc_best = accuracy_score(y_test, y_pred)\n","print(\"Best SGD - Test accuracy: {:.2f}%\".format(sgd_acc_best * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","best_sgd_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best SGD - F1 Macro score: {:.2f}\".format(best_sgd_f1_macro))\n","\n","#data processing on evaluation set\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/tfidf-balance-sgd.csv', index=False)"],"metadata":{"id":"VgiJJE5JrjLC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf balanced svm"],"metadata":{"id":"LFIKjddnrnYK"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from sklearn.utils import resample"],"metadata":{"id":"0FrD5x9JrscH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Mount Google Drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","\n","# Pre-processing\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    filtered_text = [normalize_word(w) for w in text_tokens if w not in stop_words]\n","    return \" \".join(filtered_text)\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","def stemming(data):\n","    return \" \".join([stemmer.stem(word) for word in data.split()])\n","\n","# Train data\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","# Visualizations\n","plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","# upsampling for handle class imbalance\n","majority_class = text_df[text_df['sentiment'] == 1]\n","minority_class = text_df[text_df['sentiment'] == 0]\n","\n","# Upsample minority class\n","minority_upsampled = resample(minority_class,\n","                               replace=True,\n","                               n_samples=len(majority_class),\n","                               random_state=35)\n","\n","text_df = pd.concat([majority_class, minority_upsampled])\n","\n","# Vectorize\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train/test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","# Train LinearSVC\n","SVCmodel = LinearSVC(class_weight='balanced')  # Set class_weight to balanced\n","SVCmodel.fit(x_train, y_train)\n","\n","svc_pred = SVCmodel.predict(x_test)\n","svc_acc = accuracy_score(y_test, svc_pred)\n","f1_macro = f1_score(y_test, svc_pred, average='macro')\n","\n","print(\"Test accuracy: {:.2f}%\".format(svc_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","print(confusion_matrix(y_test, svc_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, svc_pred))\n","\n","# Hyperparameter tuning\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","}\n","\n","grid = GridSearchCV(LinearSVC(class_weight='balanced'), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameter:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","logreg_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Voting Classifier - Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","best_voting_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Voting Classifier - F1 Macro score: {:.2f}\".format(best_voting_f1_macro))\n","\n","# data processing on evaluation set\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(lambda x: stemming(x))\n","\n","# Vectorize evaluation  data\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","\n","\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/tfidf-balance-svm.csv', index=False)"],"metadata":{"id":"VUJ8X9n7rttD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["bow+tfidf svm"],"metadata":{"id":"cT22X7b7r7zy"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from scipy.sparse import hstack\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","\n"],"metadata":{"id":"_M-EGGCisAS-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# pre-processing Step\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","\n","fig = plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","\n","fig = plt.figure(figsize=(7, 7))\n","colors = (\"yellowgreen\", \"red\")\n","wp = {'linewidth': 2, 'edgecolor': \"black\"}\n","tags = text_df['sentiment'].value_counts()\n","explode = (0.1, 0.1)\n","tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors=colors,\n","          startangle=90, wedgeprops=wp, explode=explode, label='')\n","plt.title('Distribution of sentiments')\n","\n","#  WordCloud for positive tweets\n","pos_tweets = text_df[text_df.sentiment == 1]\n","text = ' '.join([word for word in pos_tweets['text']])\n","plt.figure(figsize=(20, 15), facecolor='None')\n","wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.title('Most frequent words in positive tweets', fontsize=19)\n","plt.show()\n","\n","#  WordCloud for negative tweets\n","neg_tweets = text_df[text_df.sentiment == 0]\n","text = ' '.join([word for word in neg_tweets['text']])\n","plt.figure(figsize=(20, 15), facecolor='None')\n","wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.title('Most frequent words in negative tweets', fontsize=19)\n","plt.show()\n","\n","\n","count_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=15000)\n","X_tf = count_vectorizer.fit_transform(text_df['text'])\n","\n","# TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=15000)\n","X_tfidf = tfidf_vectorizer.fit_transform(text_df['text'])\n","\n","X = hstack([X_tf, X_tfidf])\n","Y = text_df['sentiment']\n","#train test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","SVCmodel = LinearSVC()\n","SVCmodel.fit(x_train, y_train)\n","svc_pred = SVCmodel.predict(x_test)\n","svc_acc = accuracy_score(y_test, svc_pred)\n","print(\"Test accuracy: {:.2f}%\".format(svc_acc * 100))\n","f1_macro = f1_score(y_test, svc_pred, average='macro')\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","\n","print(confusion_matrix(y_test, svc_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, svc_pred))\n","\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","    'class_weight': [None, 'balanced'],\n","}\n","\n","# Perform grid search\n","grid = GridSearchCV(LinearSVC(), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameter:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","logreg_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Voting Classifier - Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","best_voting_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Voting Classifier - F1 Macro score: {:.2f}\".format(best_voting_f1_macro))\n","\n","\n","#  data processing on evaluation set\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(lambda x: stemming(x))\n","\n","\n","best_model.fit(X, Y)\n","\n","\n","eval_tf = count_vectorizer.transform(eval_df['text'])\n","eval_tfidf = tfidf_vectorizer.transform(eval_df['text'])\n","\n","evaluation_test = hstack([eval_tf, eval_tfidf])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/combined-svm.csv', index=False)"],"metadata":{"id":"Owh9UXH0sAIt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["bow+tfidf naive bayse"],"metadata":{"id":"XUyEVAHvsI22"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from scipy.sparse import hstack\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","from sklearn.feature_extraction.text import CountVectorizer\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score\n","from sklearn.naive_bayes import MultinomialNB\n","from google.colab import drive\n","\n","\n"],"metadata":{"id":"-z06E5s6sO8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","\n","development_df.head()\n","\n","development_df.info()\n","\n","development_df.isnull().sum()\n","\n","development_df.columns\n","\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df.head()\n","\n","print(text_df['text'].iloc[0], \"\\n\")\n","print(text_df['text'].iloc[1], \"\\n\")\n","print(text_df['text'].iloc[2], \"\\n\")\n","print(text_df['text'].iloc[3], \"\\n\")\n","print(text_df['text'].iloc[4], \"\\n\")\n","\n","\n","# Data processing function\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming function\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","\n","\n","\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","text_df.head()\n","\n","\n","fig = plt.figure(figsize=(5,5))\n","sns.countplot(x='sentiment', data=text_df)\n","\n","\n","fig = plt.figure(figsize=(7,7))\n","colors = (\"yellowgreen\",  \"red\")\n","wp = {'linewidth':2, 'edgecolor':\"black\"}\n","tags = text_df['sentiment'].value_counts()\n","explode = (0.1, 0.1)\n","tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='')\n","plt.title('Distribution of sentiments')\n","\n","# Word cloud for positive tweets\n","pos_tweets = text_df[text_df.sentiment == 1]\n","text = ' '.join([word for word in pos_tweets['text']])\n","plt.figure(figsize=(20,15), facecolor='None')\n","wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.title('Most frequent words in positive tweets', fontsize=19)\n","plt.show()\n","\n","# Word cloud for negative tweets\n","neg_tweets = text_df[text_df.sentiment == 0]\n","text = ' '.join([word for word in neg_tweets['text']])\n","plt.figure(figsize=(20,15), facecolor='None')\n","wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.title('Most frequent words in negative tweets', fontsize=19)\n","plt.show()\n","\n","\n","count_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=15000)\n","X_tf = count_vectorizer.fit_transform(text_df['text'])\n","\n","# TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=15000)\n","X_tfidf = tfidf_vectorizer.fit_transform(text_df['text'])\n","\n","X = hstack([X_tf, X_tfidf])\n","\n","Y = text_df['sentiment']\n","\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","print(\"Size of x_train:\", (x_train.shape))\n","print(\"Size of y_train:\", (y_train.shape))\n","print(\"Size of x_test:\", (x_test.shape))\n","print(\"Size of y_test:\", (y_test.shape))\n","\n","nb_model = MultinomialNB()\n","nb_model.fit(x_train, y_train)\n","nb_pred = nb_model.predict(x_test)\n","nb_acc = accuracy_score(nb_pred, y_test)\n","nb_f1_macro = f1_score(y_test, nb_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(nb_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(nb_f1_macro))\n","\n","print(confusion_matrix(y_test, nb_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, nb_pred))\n","\n","style.use('classic')\n","cm = confusion_matrix(y_test, nb_pred, labels=nb_model.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb_model.classes_)\n","disp.plot()\n","\n","# Grid Search for hyperparameter tuning\n","param_grid = {'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]}\n","grid = GridSearchCV(MultinomialNB(), param_grid,n_jobs=-1)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameters:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","nb_acc = accuracy_score(y_pred, y_test)\n","nb_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Test accuracy after Grid Search: {:.2f}%\".format(nb_acc * 100))\n","print(\"F1 Macro score after Grid Search: {:.2f}\".format(nb_f1_macro))\n","\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","\n","\n","#  data processing\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(stemming)\n","\n","\n","best_model.fit(X, Y)\n","\n","# Vectorize evaluation text data\n","eval_tf = count_vectorizer.transform(eval_df['text'])\n","eval_tfidf = tfidf_vectorizer.transform(eval_df['text'])\n","\n","evaluation_test = hstack([eval_tf, eval_tfidf])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/combined-naivebayse.csv', index=False)"],"metadata":{"id":"Ntt0n5S0sPM0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["bow+tfidf logistic-regression"],"metadata":{"id":"XeHMsqIVsXl8"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from scipy.sparse import hstack\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from google.colab import drive\n","from wordcloud import WordCloud\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n"],"metadata":{"id":"IYVC7ktVscVt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","\n","drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# Data processing function\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming function\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","print(development_df.head())\n","print(development_df.info())\n","print(development_df.isnull().sum())\n","\n","\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(stemming)\n","\n","fig = plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","# Create word clouds\n","def create_wordcloud(text, title):\n","    plt.figure(figsize=(10, 8))\n","    wordcloud = WordCloud(max_words=100, background_color='white').generate(text)\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis('off')\n","    plt.title(title)\n","    plt.show()\n","\n","# Positive tweets\n","pos_tweets = text_df[text_df.sentiment == 1]\n","create_wordcloud(' '.join(pos_tweets['text']), 'Most Frequent Words in Positive Tweets')\n","\n","# Negative tweets\n","neg_tweets = text_df[text_df.sentiment == 0]\n","create_wordcloud(' '.join(neg_tweets['text']), 'Most Frequent Words in Negative Tweets')\n","\n","# Vectorization\n","count_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=15000)\n","X_tf = count_vectorizer.fit_transform(text_df['text'])\n","\n","# TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=15000)\n","X_tfidf = tfidf_vectorizer.fit_transform(text_df['text'])\n","\n","X = hstack([X_tf, X_tfidf])\n","Y = text_df['sentiment']\n","\n","# train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n","logreg.fit(x_train, y_train)\n","\n","logreg_pred = logreg.predict(x_test)\n","logreg_acc = accuracy_score(y_test, logreg_pred)\n","f1_macro = f1_score(y_test, logreg_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","\n","print(confusion_matrix(y_test, logreg_pred))\n","print(classification_report(y_test, logreg_pred))\n","\n","\n","cm = confusion_matrix(y_test, logreg_pred, labels=logreg.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\n","disp.plot()\n","\n","# Hyperparameter tuning with Grid Search\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n","grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameters:\", grid.best_params_)\n","\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","logreg_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Voting Classifier - Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","best_voting_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Voting Classifier - F1 Macro score: {:.2f}\".format(best_voting_f1_macro))\n","\n","#  data processing on the evaluation set\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(stemming)\n","\n","best_model.fit(X, Y)\n","\n","# Vectorize evaluation  data\n","eval_tf = count_vectorizer.transform(eval_df['text'])\n","eval_tfidf = tfidf_vectorizer.transform(eval_df['text'])\n","\n","\n","evaluation_test = hstack([eval_tf, eval_tfidf])\n","\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/combined-logisticRegression.csv', index=False)"],"metadata":{"id":"eJZUfFrAscje"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["bow+tfidf  sgd"],"metadata":{"id":"-8PR-0haslTh"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from scipy.sparse import hstack\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","import warnings\n","from sklearn.linear_model import SGDClassifier\n"],"metadata":{"id":"PFvT2XXusqfG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","\n","# pre-processing Step\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","# data processing\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","# Vectorize\n","count_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=15000)\n","X_tf = count_vectorizer.fit_transform(text_df['text'])\n","\n","# TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=15000)\n","X_tfidf = tfidf_vectorizer.fit_transform(text_df['text'])\n","\n","X = hstack([X_tf, X_tfidf])\n","Y = text_df['sentiment']\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","sgd_clf = SGDClassifier(loss='hinge', random_state=35)\n","sgd_clf.fit(x_train, y_train)\n","y_pred = sgd_clf.predict(x_test)\n","sgd_acc = accuracy_score(y_test, y_pred)\n","sgd_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(sgd_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(sgd_f1_macro))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","style.use('classic')\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot()\n","\n","param_grid = {\n","    'loss': ['hinge', 'log_loss', 'squared_hinge', 'modified_huber'],\n","    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n","    'penalty': ['l2', 'l1', 'elasticnet'],\n","    'max_iter': [1000, 2000]\n","}\n","\n","grid_search = GridSearchCV(estimator=sgd_clf, param_grid=param_grid, scoring='f1_macro', cv=3, n_jobs=-1)\n","grid_search.fit(x_train, y_train)\n","\n","print(\"Best parameters from Grid Search:\", grid_search.best_params_)\n","best_model = grid_search.best_estimator_\n","y_pred_best = best_model.predict(x_test)\n","best_sgd_acc = accuracy_score(y_test, y_pred_best)\n","best_sgd_f1_macro = f1_score(y_test, y_pred_best, average='macro')\n","\n","print(\"Best SGD Classifier - Test accuracy: {:.2f}%\".format(best_sgd_acc * 100))\n","print(\"Best SGD Classifier - F1 Macro score: {:.2f}\".format(best_sgd_f1_macro))\n","cm_best = confusion_matrix(y_test, y_pred_best)\n","disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best)\n","disp_best.plot()\n","\n","# data processing to evaluation dataset\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(stemming)\n","\n","best_model.fit(X, Y)\n","\n","\n","eval_tf = count_vectorizer.transform(eval_df['text'])\n","eval_tfidf = tfidf_vectorizer.transform(eval_df['text'])\n","\n","\n","evaluation_test = hstack([eval_tf, eval_tfidf])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/combined-sgd.csv', index=False)"],"metadata":{"id":"SCiaKxBqsp81"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf xgboost"],"metadata":{"id":"nG0ST_0ys6zT"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","import xgboost as xgb\n","from google.colab import drive\n","import joblib\n","from joblib import Parallel, delayed\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score"],"metadata":{"id":"uwTWwH0PtJg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# processing function\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = Parallel(n_jobs=-1)(delayed(data_processing)(text) for text in text_df['text'])\n","text_df['text'] = Parallel(n_jobs=-1)( delayed(stemming)(tokens) for tokens in text_df['text'] )\n","\n","# vectorizer = CountVectorizer(max_features=2048)\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=2048)  # Reduced max_features\n","X = vectorizer.fit_transform(development_df['text'])\n","Y = development_df['sentiment']\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","xgb_model = xgb.XGBClassifier(max_depth=6, random_state=42, n_estimators=1000, use_label_encoder=False, eval_metric='mlogloss', nthread=3)\n","xgb_model.fit(x_train, y_train)\n","\n","xgb_pred = xgb_model.predict(x_test)\n","\n","xgb_acc = accuracy_score(y_test, xgb_pred)\n","xgb_f1_macro = f1_score(y_test, xgb_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(xgb_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(xgb_f1_macro))\n","print(confusion_matrix(y_test, xgb_pred))\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, xgb_pred))\n","\n","cm = confusion_matrix(y_test, xgb_pred, labels=xgb_model.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb_model.classes_)\n","disp.plot()\n","\n","# Random Search for hyperparameter tuning\n","param_dist = {\n","    'n_estimators': [100, 200, 500],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'max_depth': [5, 6, 7, 8],\n","}\n","random_search = RandomizedSearchCV(xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss', nthread=3),\n","                                   param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1, random_state=35)\n","random_search.fit(x_train, y_train)\n","\n","print(\"Best parameters:\", random_search.best_params_)\n","\n","\n","best_model = random_search.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","xgb_acc = accuracy_score(y_test, y_pred)\n","xgb_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Test accuracy after Random Search: {:.2f}%\".format(xgb_acc * 100))\n","print(\"F1 Macro score after Random Search: {:.2f}\".format(xgb_f1_macro))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","\n","# Data processing on evaluation data\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = Parallel(n_jobs=-1)(delayed(data_processing)(text) for text in eval_df['text'])\n","eval_df['text'] = Parallel(n_jobs=-1)( delayed(stemming)(tokens) for tokens in eval_df['text'] )\n","\n","evaluation_test = vectorizer.transform(evaluation_df['text'])\n","best_model.fit(X, Y)\n","\n","\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/fulldata-tfidf-xgboost.csv', index=False)"],"metadata":{"id":"QTls5feCtJv3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf Voting Classifier(Logistic Regression+ Stochastic Gradient Descent)"],"metadata":{"id":"tT9d_z8PtSlx"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score\n","from sklearn.naive_bayes import MultinomialNB\n","from google.colab import drive\n","\n"],"metadata":{"id":"cNLU3eg7tXwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# Data processing function\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","\n","\n","#  data processing\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","\n","# vectorizer = CountVectorizer(ngram_range=(1, 2))\n","vectorizer=TfidfVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","logreg = LogisticRegression(random_state=42,max_iter=10000)\n","sgd = SGDClassifier(random_state=42,loss='log_loss',max_iter=10000)\n","voting_clf = VotingClassifier(estimators=[('logreg', logreg), ('sgd', sgd)], voting='soft')\n","voting_clf.fit(x_train, y_train)\n","y_pred = voting_clf.predict(x_test)\n","\n","voting_acc = accuracy_score(y_test, y_pred)\n","voting_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(voting_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(voting_f1_macro))\n","\n","\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","\n","style.use('classic')\n","cm = confusion_matrix(y_test, y_pred, labels=voting_clf.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=voting_clf.classes_)\n","disp.plot()\n","\n","\n","\n","# parameter grid for Grid Search\n","param_grid = {\n","    'logreg__C': [0.001, 0.01, 0.1, 1, 10],\n","    'sgd__alpha': [0.0001, 0.001, 0.01, 0.1]\n","}\n","\n","grid_search = GridSearchCV(estimator=voting_clf, param_grid=param_grid, scoring='f1_macro', cv=3)\n","grid_search.fit(x_train, y_train)\n","\n","print(\"Best parameters from Grid Search:\", grid_search.best_params_)\n","best_model = grid_search.best_estimator_\n","y_pred_best = best_model.predict(x_test)\n","best_voting_acc = accuracy_score(y_test, y_pred_best)\n","best_voting_f1_macro = f1_score(y_test, y_pred_best, average='macro')\n","\n","print(\"Best Voting Classifier - Test accuracy: {:.2f}%\".format(best_voting_acc * 100))\n","print(\"Best Voting Classifier - F1 Macro score: {:.2f}\".format(best_voting_f1_macro))\n","\n","cm_best = confusion_matrix(y_test, y_pred_best)\n","disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best, display_labels=best_model.classes_)\n","disp_best.plot()\n","\n","\n","# data processing\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(stemming)\n","\n","best_model.fit(X, Y)\n","\n","eveluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(eveluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('fulldata-tfidf-LrSgd.csv', index=False)"],"metadata":{"id":"51a8Th3LtYXe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf svm"],"metadata":{"id":"uMCcXojIthjL"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE"],"metadata":{"id":"Ob_PVyLntmGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# pre-processing Step\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","# train data\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","\n","fig = plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","\n","fig = plt.figure(figsize=(7, 7))\n","colors = (\"yellowgreen\", \"red\")\n","wp = {'linewidth': 2, 'edgecolor': \"black\"}\n","tags = text_df['sentiment'].value_counts()\n","explode = (0.1, 0.1)\n","tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors=colors,\n","          startangle=90, wedgeprops=wp, explode=explode, label='')\n","plt.title('Distribution of sentiments')\n","\n","#  WordCloud for positive tweets\n","pos_tweets = text_df[text_df.sentiment == 1]\n","text = ' '.join([word for word in pos_tweets['text']])\n","plt.figure(figsize=(20, 15), facecolor='None')\n","wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.title('Most frequent words in positive tweets', fontsize=19)\n","plt.show()\n","\n","#  WordCloud for negative tweets\n","neg_tweets = text_df[text_df.sentiment == 0]\n","text = ' '.join([word for word in neg_tweets['text']])\n","plt.figure(figsize=(20, 15), facecolor='None')\n","wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.title('Most frequent words in negative tweets', fontsize=19)\n","plt.show()\n","\n","# Vectorize\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=15000)\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","#train test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","SVCmodel = LinearSVC()\n","SVCmodel.fit(x_train, y_train)\n","svc_pred = SVCmodel.predict(x_test)\n","svc_acc = accuracy_score(y_test, svc_pred)\n","print(\"Test accuracy: {:.2f}%\".format(svc_acc * 100))\n","f1_macro = f1_score(y_test, svc_pred, average='macro')\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","\n","print(confusion_matrix(y_test, svc_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, svc_pred))\n","\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","    'class_weight': [None, 'balanced'],\n","}\n","\n","# Perform grid search\n","grid = GridSearchCV(LinearSVC(), param_grid, cv=5, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameter:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","logreg_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Voting Classifier - Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","best_voting_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Voting Classifier - F1 Macro score: {:.2f}\".format(best_voting_f1_macro))\n","\n","\n","#  data processing on evaluation set\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(lambda x: stemming(x))\n","\n","\n","best_model.fit(X, Y)\n","\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('fulldata_tfidf-linearSVM.csv', index=False)"],"metadata":{"id":"osfQlpX2tmm4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf sgd"],"metadata":{"id":"pHDke2c-tweA"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.svm import LinearSVC\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n","from imblearn.over_sampling import SMOTE\n","import warnings\n","from sklearn.linear_model import SGDClassifier"],"metadata":{"id":"ezxbBNazt1CR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","drive.mount('/content/drive/')\n","\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","\n","# pre-processing Step\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","# data processing\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","#  TfidfVectorizer\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","sgd_clf = SGDClassifier(loss='hinge', random_state=35)\n","sgd_clf.fit(x_train, y_train)\n","y_pred = sgd_clf.predict(x_test)\n","\n","\n","sgd_acc = accuracy_score(y_test, y_pred)\n","sgd_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(sgd_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(sgd_f1_macro))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","style.use('classic')\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot()\n","\n","param_grid = {\n","    'loss': ['hinge', 'log_loss', 'squared_hinge', 'modified_huber'],\n","    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n","    'penalty': ['l2', 'l1', 'elasticnet'],\n","    'max_iter': [1000, 2000]\n","}\n","\n","grid_search = GridSearchCV(estimator=sgd_clf, param_grid=param_grid, scoring='f1_macro', cv=3, n_jobs=-1)\n","grid_search.fit(x_train, y_train)\n","\n","print(\"Best parameters from Grid Search:\", grid_search.best_params_)\n","\n","best_model = grid_search.best_estimator_\n","y_pred_best = best_model.predict(x_test)\n","best_sgd_acc = accuracy_score(y_test, y_pred_best)\n","best_sgd_f1_macro = f1_score(y_test, y_pred_best, average='macro')\n","\n","print(\"Best SGD Classifier - Test accuracy: {:.2f}%\".format(best_sgd_acc * 100))\n","print(\"Best SGD Classifier - F1 Macro score: {:.2f}\".format(best_sgd_f1_macro))\n","\n","cm_best = confusion_matrix(y_test, y_pred_best)\n","disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best)\n","disp_best.plot()\n","\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(stemming)\n","\n","best_model.fit(X, Y)\n","\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/full-tfidf-sgd1.csv', index=False)"],"metadata":{"id":"_rY4JVbat1Uo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf random forest"],"metadata":{"id":"8mDgFsdouAJn"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from joblib import Parallel, delayed\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","import xgboost as xgb\n","from google.colab import drive\n","import joblib\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score"],"metadata":{"id":"h33ES0AmuEj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# Data processing function\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming function\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]  t\n","    return \" \".join(text)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = Parallel(n_jobs=-1)(delayed(data_processing)(text) for text in text_df['text'])\n","text_df['text'] = Parallel(n_jobs=-1)( delayed(stemming)(tokens) for tokens in text_df['text'] )\n","\n","# vectorizer = CountVectorizer(ngram_range=(1, 2),max_features=4096)\n","# vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=4096)\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","rf_model = RandomForestClassifier(random_state=42)\n","rf_model.fit(x_train, y_train)\n","rf_pred = rf_model.predict(x_test)\n","print(f\"Test accuracy: {accuracy_score(y_test, rf_pred) * 100:.2f}%\")\n","print(f\"F1 Macro score: {f1_score(y_test, rf_pred, average='macro'):.2f}\")\n","print(confusion_matrix(y_test, rf_pred))\n","print(classification_report(y_test, rf_pred))\n","\n","# Grid Search for hyperparameter tuning\n","param_grid = {\n","    'n_estimators': [50, 100],\n","    'max_depth': [10, 20],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2]\n","}\n","grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameters:\", grid.best_params_)\n","\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","print(f\"Test accuracy after Grid Search: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n","print(f\"F1 Macro score after Grid Search: {f1_score(y_test, y_pred, average='macro'):.2f}\")\n","\n","# Data processing on evaluation data\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = Parallel(n_jobs=-1)(delayed(data_processing)(text) for text in eval_df['text'])\n","eval_df['text'] = Parallel(n_jobs=-1)( delayed(stemming)(tokens) for tokens in eval_df['text'] )\n","\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","best_model.fit(X, Y)\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/fulldata-tfidf-randomforest4096.csv', index=False)\n"],"metadata":{"id":"06WUZq0HuFHa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf naive bayse"],"metadata":{"id":"OD97tHaeuStk"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score\n","from sklearn.naive_bayes import MultinomialNB\n","from google.colab import drive"],"metadata":{"id":"d3JISTs8uZgV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","\n","development_df.head()\n","\n","development_df.info()\n","\n","development_df.isnull().sum()\n","\n","development_df.columns\n","\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df.head()\n","\n","print(text_df['text'].iloc[0], \"\\n\")\n","print(text_df['text'].iloc[1], \"\\n\")\n","print(text_df['text'].iloc[2], \"\\n\")\n","print(text_df['text'].iloc[3], \"\\n\")\n","print(text_df['text'].iloc[4], \"\\n\")\n","\n","\n","# Data processing function\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","\n","\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","text_df.head()\n","\n","fig = plt.figure(figsize=(5,5))\n","sns.countplot(x='sentiment', data=text_df)\n","\n","fig = plt.figure(figsize=(7,7))\n","colors = (\"yellowgreen\",  \"red\")\n","wp = {'linewidth':2, 'edgecolor':\"black\"}\n","tags = text_df['sentiment'].value_counts()\n","explode = (0.1, 0.1)\n","tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='')\n","plt.title('Distribution of sentiments')\n","\n","# Word cloud for positive tweets\n","pos_tweets = text_df[text_df.sentiment == 1]\n","text = ' '.join([word for word in pos_tweets['text']])\n","plt.figure(figsize=(20,15), facecolor='None')\n","wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.title('Most frequent words in positive tweets', fontsize=19)\n","plt.show()\n","\n","# Word cloud for negative tweets\n","neg_tweets = text_df[text_df.sentiment == 0]\n","text = ' '.join([word for word in neg_tweets['text']])\n","plt.figure(figsize=(20,15), facecolor='None')\n","wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.title('Most frequent words in negative tweets', fontsize=19)\n","plt.show()\n","\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","vect = vectorizer.fit(text_df['text'])\n","\n","feature_names = vectorizer.get_feature_names_out()\n","print(\"Number of features: {}\\n\".format(len(feature_names)))\n","print(\"First 20 features:\\n {}\".format(feature_names[:20]))\n","\n","X = text_df['text']\n","Y = text_df['sentiment']\n","X = vect.transform(X)\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","print(\"Size of x_train:\", (x_train.shape))\n","print(\"Size of y_train:\", (y_train.shape))\n","print(\"Size of x_test:\", (x_test.shape))\n","print(\"Size of y_test:\", (y_test.shape))\n","nb_model = MultinomialNB()\n","nb_model.fit(x_train, y_train)\n","nb_pred = nb_model.predict(x_test)\n","nb_acc = accuracy_score(nb_pred, y_test)\n","nb_f1_macro = f1_score(y_test, nb_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(nb_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(nb_f1_macro))\n","print(confusion_matrix(y_test, nb_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, nb_pred))\n","style.use('classic')\n","cm = confusion_matrix(y_test, nb_pred, labels=nb_model.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb_model.classes_)\n","disp.plot()\n","\n","# Grid Search for hyperparameter tuning\n","param_grid = {'alpha': [0.1, 0.5, 1.0, 1.5, 2.0]}\n","grid = GridSearchCV(MultinomialNB(), param_grid,n_jobs=-1)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameters:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","nb_acc = accuracy_score(y_pred, y_test)\n","nb_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Test accuracy after Grid Search: {:.2f}%\".format(nb_acc * 100))\n","print(\"F1 Macro score after Grid Search: {:.2f}\".format(nb_f1_macro))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","\n","\n","\n","#  data processing\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(stemming)\n","best_model.fit(X, Y)\n","eveluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(eveluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('full-tfidf-naivebayse.csv', index=False)"],"metadata":{"id":"Ige2XhGruZ9y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf logistic-regression"],"metadata":{"id":"12H9CD01ukrt"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from google.colab import drive\n","from wordcloud import WordCloud\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n"],"metadata":{"id":"_nxzsUkOuq7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount Google Drive\n","\n","drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# Data processing\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","def normalize_word(word):\n","     return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","print(development_df.head())\n","print(development_df.info())\n","print(development_df.isnull().sum())\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(stemming)\n","\n","\n","fig = plt.figure(figsize=(5, 5))\n","sns.countplot(x='sentiment', data=text_df)\n","plt.title('Sentiment Distribution')\n","plt.show()\n","\n","#  word clouds\n","def create_wordcloud(text, title):\n","    plt.figure(figsize=(10, 8))\n","    wordcloud = WordCloud(max_words=100, background_color='white').generate(text)\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis('off')\n","    plt.title(title)\n","    plt.show()\n","\n","# Positive tweets\n","pos_tweets = text_df[text_df.sentiment == 1]\n","create_wordcloud(' '.join(pos_tweets['text']), 'Most Frequent Words in Positive Tweets')\n","\n","# Negative tweets\n","neg_tweets = text_df[text_df.sentiment == 0]\n","create_wordcloud(' '.join(neg_tweets['text']), 'Most Frequent Words in Negative Tweets')\n","\n","# Vectorization using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","X = tfidf_vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","logreg = LogisticRegression(max_iter=1000, class_weight='balanced')\n","logreg.fit(x_train, y_train)\n","\n","logreg_pred = logreg.predict(x_test)\n","logreg_acc = accuracy_score(y_test, logreg_pred)\n","f1_macro = f1_score(y_test, logreg_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(f1_macro))\n","\n","print(confusion_matrix(y_test, logreg_pred))\n","print(classification_report(y_test, logreg_pred))\n","\n","cm = confusion_matrix(y_test, logreg_pred, labels=logreg.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\n","disp.plot()\n","\n","# Hyperparameter tuning with Grid Search\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n","grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, n_jobs=-1)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameters:\", grid.best_params_)\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","logreg_acc = accuracy_score(y_test, y_pred)\n","print(\"Best Voting Classifier - Test accuracy: {:.2f}%\".format(logreg_acc * 100))\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","best_voting_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Best Voting Classifier - F1 Macro score: {:.2f}\".format(best_voting_f1_macro))\n","\n","#  data processing on the evaluation set\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(stemming)\n","best_model.fit(X, Y)\n","evaluation_test = tfidf_vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/tfidf-logisticRegression_full_data_predictions.csv', index=False)\n"],"metadata":{"id":"8w_Rf9YsurUH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf Gradient Boosting Machine"],"metadata":{"id":"7q7iCZuju4Vi"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","import xgboost as xgb\n","from google.colab import drive\n","import joblib\n","from joblib import Parallel, delayed\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score"],"metadata":{"id":"3f3Rv_dFu_74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# Data processing\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = Parallel(n_jobs=-1)(delayed(data_processing)(text) for text in text_df['text'])\n","text_df['text'] = Parallel(n_jobs=-1)( delayed(stemming)(tokens) for tokens in text_df['text'] )\n","\n","\n","# vectorizer = CountVectorizer(ngram_range=(1, 2),max_features=4096)\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=4096)\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","gb_clf = GradientBoostingClassifier(random_state=35)\n","gb_clf.fit(x_train, y_train)\n","y_pred = gb_clf.predict(x_test)\n","\n","gb_acc = accuracy_score(y_test, y_pred)\n","gb_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(gb_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(gb_f1_macro))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","style.use('classic')\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot()\n","\n","# Grid Search for hyperparameter tuning\n","param_grid = {\n","    'n_estimators': [50, 100, 150, 200],\n","    'learning_rate': [0.001, 0.01, 0.1, 1],\n","    'max_depth': [3, 5, 7, 9]\n","}\n","\n","grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, scoring='f1_macro', cv=3,n_jobs=-1)\n","grid_search.fit(x_train, y_train)\n","\n","print(\"Best parameters from Grid Search:\", grid_search.best_params_)\n","\n","\n","best_model = grid_search.best_estimator_\n","y_pred_best = best_model.predict(x_test)\n","\n","best_gb_acc = accuracy_score(y_test, y_pred_best)\n","best_gb_f1_macro = f1_score(y_test, y_pred_best, average='macro')\n","\n","print(\"Best Gradient Boosting Classifier - Test accuracy: {:.2f}%\".format(best_gb_acc * 100))\n","print(\"Best Gradient Boosting Classifier - F1 Macro score: {:.2f}\".format(best_gb_f1_macro))\n","\n","cm_best = confusion_matrix(y_test, y_pred_best)\n","disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best)\n","disp_best.plot()\n","\n","# Data processing on evaluation data\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = Parallel(n_jobs=-1)(delayed(data_processing)(text) for text in eval_df['text'])\n","eval_df['text'] = Parallel(n_jobs=-1)( delayed(stemming)(tokens) for tokens in eval_df['text'] )\n","eveluation_test = vectorizer.transform(eval_df['text'])\n","best_model.fit(X, Y)\n","predictions = best_model.predict(eveluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/fulldata-tfidf-gbm.csv', index=False)"],"metadata":{"id":"uodegK-UvAJF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf extra tree"],"metadata":{"id":"5Knu_-WvvLXP"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score\n","from sklearn.naive_bayes import MultinomialNB\n","from google.colab import drive\n","from joblib import Parallel, delayed"],"metadata":{"id":"f17RYVhevRae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# Data processing\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = Parallel(n_jobs=-1)(delayed(data_processing)(text) for text in text_df['text'])\n","text_df['text'] = Parallel(n_jobs=-1)( delayed(stemming)(tokens) for tokens in text_df['text'] )\n","\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=2048)\n","X = vectorizer.fit_transform(text_df['text'])\n","Y = text_df['sentiment']\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","extra_tree_clf = ExtraTreesClassifier(random_state=35, n_jobs=-1)\n","\n","# Hyperparameter tuning with RandomizedSearchCV\n","param_distributions = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","rand_search = RandomizedSearchCV(estimator=extra_tree_clf, param_distributions=param_distributions,\n","                                 scoring='f1_macro', n_iter=10, cv=3, n_jobs=-1, random_state=35)\n","rand_search.fit(x_train, y_train)\n","\n","print(\"Best parameters from Randomized Search:\", rand_search.best_params_)\n","best_model = rand_search.best_estimator_\n","y_pred_best = best_model.predict(x_test)\n","best_extra_tree_acc = accuracy_score(y_test, y_pred_best)\n","best_extra_tree_f1_macro = f1_score(y_test, y_pred_best, average='macro')\n","\n","print(\"Best Extra Trees Classifier - Test accuracy: {:.2f}%\".format(best_extra_tree_acc * 100))\n","print(\"Best Extra Trees Classifier - F1 Macro score: {:.2f}\".format(best_extra_tree_f1_macro))\n","\n","print(confusion_matrix(y_test, y_pred_best))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred_best))\n","\n","cm_best = confusion_matrix(y_test, y_pred_best)\n","disp_best = ConfusionMatrixDisplay(confusion_matrix=cm_best)\n","disp_best.plot()\n","plt.title(\"Best Model Confusion Matrix\")\n","plt.show()\n","\n","# Data processing on evaluation data\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = Parallel(n_jobs=-1)(delayed(data_processing)(text) for text in eval_df['text'])\n","eval_df['text'] = Parallel(n_jobs=-1)( delayed(stemming)(tokens) for tokens in eval_df['text'] )\n","\n","evaluation_test = vectorizer.transform(eval_df['text'])\n","\n","best_model.fit(X, Y)\n","predictions = best_model.predict(evaluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('/content/drive/MyDrive/fulldata-tfidf-etc.csv', index=False)"],"metadata":{"id":"-WcbuGvUvSpA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["tfidf decision tree"],"metadata":{"id":"HVK-XEYovkcJ"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import f1_score\n","from imblearn.over_sampling import SMOTE\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import xgboost as xgb\n","import matplotlib.pyplot as plt\n","from scipy.sparse import csr_matrix, hstack, vstack\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","import seaborn as sns\n","from matplotlib import style\n","style.use('ggplot')\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score\n","from sklearn.naive_bayes import MultinomialNB\n","from google.colab import drive"],"metadata":{"id":"uVPhxxexvmrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive/')\n","# Load dataset\n","development_path = \"/content/drive/MyDrive/dsl-fall/development.csv\"\n","development_df = pd.read_csv(development_path)\n","evaluation_path = \"/content/drive/MyDrive/dsl-fall/evaluation.csv\"\n","evaluation_df = pd.read_csv(evaluation_path)\n","\n","# Data processing function\n","def data_processing(text):\n","    text = text.lower()\n","    text = re.sub(r\"https\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text_tokens = word_tokenize(text)\n","    text_tokens = [normalize_word(w) for w in text_tokens]\n","    filtered_text = [w for w in text_tokens if not w in stop_words]\n","    return \" \".join(filtered_text)\n","\n","lemmatizer = WordNetLemmatizer()\n","nltk.download('wordnet')\n","\n","def normalize_word(word):\n","    return re.sub(r'(.)\\1+', r'\\1', word)\n","\n","\n","# # Function to clean and preprocess the text\n","# def data_processing(text):\n","#     # Remove HTML entities\n","#     soup = BeautifulSoup(text, 'lxml')\n","#     text = soup.get_text()\n","\n","#     # Remove mentions\n","#     text = re.sub(r'@[A-Za-z0-9]+', '', text)\n","\n","#     # Remove URLs\n","#     text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)\n","\n","#     # Remove punctuation and numbers and handle unnecessary whitespaces\n","#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","#     text = re.sub(r'\\s+', ' ', text).strip()\n","\n","#     # Tokenize the text\n","#     words = text.split()\n","#     # Remove stop words and meaningless words\n","#     meaningless_words = {'&lt', '&quot', '&amp', '&gt'}  # Add more if necessary\n","#     processed_words = []\n","#     for word in words:\n","#         normalized_word = normalize_word(word.lower())\n","#         if normalized_word not in stop_words and normalized_word not in meaningless_words:\n","#             # Apply stemming and lemmatization\n","#             stemmed_word = stemmer.stem(normalized_word)\n","#             lemmatized_word = lemmatizer.lemmatize(stemmed_word)\n","#             processed_words.append(lemmatized_word)\n","\n","#     return \" \".join(processed_words)\n","\n","\n","\n","# Stemming\n","stemmer = PorterStemmer()\n","def stemming(data):\n","    text = [stemmer.stem(word) for word in data.split()]\n","    return \" \".join(text)\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","\n","text_df = development_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","text_df['text'] = text_df['text'].apply(data_processing)\n","text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n","\n","# Vectorize text data\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n","# CountVectorizer(ngram_range=(1,2),max_features=4096)\n","vect = vectorizer.fit(text_df['text'])\n","\n","\n","X = text_df['text']\n","Y = text_df['sentiment']\n","X = vect.transform(X)\n","\n","# Train-test split\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=35)\n","\n","print(\"Size of x_train:\", (x_train.shape))\n","print(\"Size of y_train:\", (y_train.shape))\n","print(\"Size of x_test:\", (x_test.shape))\n","print(\"Size of y_test:\", (y_test.shape))\n","\n","dt_model = DecisionTreeClassifier(random_state=35)\n","dt_model.fit(x_train, y_train)\n","dt_pred = dt_model.predict(x_test)\n","dt_acc = accuracy_score(dt_pred, y_test)\n","dt_f1_macro = f1_score(y_test, dt_pred, average='macro')\n","print(\"Test accuracy: {:.2f}%\".format(dt_acc * 100))\n","print(\"F1 Macro score: {:.2f}\".format(dt_f1_macro))\n","print(confusion_matrix(y_test, dt_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, dt_pred))\n","style.use('classic')\n","cm = confusion_matrix(y_test, dt_pred, labels=dt_model.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dt_model.classes_)\n","disp.plot()\n","\n","# Grid Search for hyperparameter tuning\n","param_grid = {\n","    'max_depth': [10, 20, 30, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid)\n","grid.fit(x_train, y_train)\n","\n","print(\"Best parameters:\", grid.best_params_\n","best_model = grid.best_estimator_\n","y_pred = best_model.predict(x_test)\n","\n","dt_acc = accuracy_score(y_pred, y_test)\n","dt_f1_macro = f1_score(y_test, y_pred, average='macro')\n","print(\"Test accuracy after Grid Search: {:.2f}%\".format(dt_acc * 100))\n","print(\"F1 Macro score after Grid Search: {:.2f}\".format(dt_f1_macro))\n","print(confusion_matrix(y_test, y_pred))\n","print(\"\\n\")\n","print(classification_report(y_test, y_pred))\n","#  data processing\n","eval_df = evaluation_df.drop(['ids', 'date', 'flag', 'user'], axis=1)\n","eval_df['text'] = eval_df['text'].apply(data_processing)\n","eval_df['text'] = eval_df['text'].apply(stemming)\n","\n","best_model.fit(X, Y)\n","\n","eveluation_test = vectorizer.transform(eval_df['text'])\n","predictions = best_model.predict(eveluation_test)\n","\n","# Save predictions to CSV\n","submission = pd.DataFrame({'Id': evaluation_df['ids'], 'Predicted': predictions})\n","submission.to_csv('bow-DT.csv', index=False)"],"metadata":{"id":"ohZ0ULWdvnuM"},"execution_count":null,"outputs":[]}]}